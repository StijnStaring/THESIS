Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Palan2019,
abstract = {Our goal is to accurately and efficiently learn reward functions for autonomous robots. Current approaches to this problem include inverse reinforcement learning (IRL), which uses expert demonstrations, and preference-based learning, which iteratively queries the user for her preferences between trajectories. In robotics however, IRL often struggles because it is difficult to get high-quality demonstrations; conversely, preference-based learning is very inefficient since it attempts to learn a continuous, high-dimensional function from binary feedback. We propose a new framework for reward learning, DemPref, that uses both demonstrations and preference queries to learn a reward function. Specifically, we (1) use the demonstrations to learn a coarse prior over the space of reward functions, to reduce the effective size of the space from which queries are generated; and (2) use the demonstrations to ground the (active) query generation process, to improve the quality of the generated queries. Our method alleviates the efficiency issues faced by standard preference-based learning methods and does not exclusively depend on (possibly low-quality) demonstrations. In numerical experiments, we find that DemPref is significantly more efficient than a standard active preference-based learning method. In a user study, we compare our method to a standard IRL method; we find that users rated the robot trained with DemPref as being more successful at learning their desired behavior, and preferred to use the DemPref system (over IRL) to train the robot.},
archivePrefix = {arXiv},
arxivId = {1906.08928},
author = {Palan, Malayandi and Landolfi, Nicholas C. and Shevchuk, Gleb and Sadigh, Dorsa},
eprint = {1906.08928},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/obesvation{\_}based{\_}learning/Learning reward functions by integrating human demonstrations and preferences.pdf:pdf},
title = {{Learning Reward Functions by Integrating Human Demonstrations and Preferences}},
url = {http://arxiv.org/abs/1906.08928},
year = {2019}
}
@article{Yankov,
abstract = {Path planning and control of autonomous vehicles in dynamic environments can be a challenging problem, as a result of complex vehicle dynamics and constraints, as well as the existence of moving obstacles. Typical trajectory planners involve a high level decision making approach, using graph searches and a prior knowledge of the environment. Other approaches, attempt to use Potential Field (PF) functions as a basis for describing a goal within the environment, applied primarily to the field of robotics. This thesis aims to develop a method for path planning using online lane detection data from sensors. A Model Predictive Control (MPC) framework is proposed to fulfill this task. The path planner uses a simple dynamic model in combination with PF functions in order to create a feasible trajectory though the environment. To ensure safety around obstacles, vehicles and other actors are modeled as a potential field. A fast MPC controller is developed for path following. The two MPCs are then integrated in a hierarchical control structure. The method has been simulated in different scenarios and validated using a 15 degrees of freedom car model},
author = {Yankov, Kaloyan},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/Stick{\_}Tong{\_}kickoff/Master{\_}Thesis{\_}Kaloyan{\_}EPFL.pdf:pdf},
pages = {60},
title = {{Potential Field Based Model Predictive Control for Autonomous Vehicle Motion Planning and Control}}
}
@article{Fakultat2018,
author = {Bellem, Hanna},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/Comfort/Comfort in autonomous driving.pdf:pdf},
title = {{Comfort in Automated Driving : Analysis of Driving Style Preference in Automated Driving}},
year = {2018}
}
@article{BrianD.ZiebartAndrewMaasJ.AndrewBagnell2008,
abstract = {Recent research has shown the benefit of framing problems of imitation learning as solutions to Markov Decision Prob- lems. This approach reduces learning to the problem of re- covering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behav- ior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-defined, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods. We develop our technique in the context of modeling real- world navigation and driving behaviors where collected data is inherently noisy and imperfect. Our probabilistic approach enables modeling of route preferences as well as a powerful new approach to inferring destinations and routes based on partial trajectories. Introduction},
author = {{Brian D. Ziebart, Andrew Maas, J.Andrew Bagnell}, and Anind K. Dey},
doi = {10.1007/978-3-662-49390-8_64},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/Inverse reinforced learning/Max entropie IRL.pdf:pdf},
isbn = {9783662493892},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {6},
title = {{Maximum Entropy Inverse Reinforcement Learning Brian}},
year = {2008}
}
@article{TongDuySon2019,
abstract = {This paper presents a novel control framework to handle safety-critical control for non-affine nonlinear systems. The proposed control development is considered to deal l with safety-critical aspects in autonomous vehicle driving. The safety constraints are guaranteed using control barrier function (CBF), which implies forward-invariance of a safe set. In particular, we focus on CBF that enforces strict statedependent high relative degree constraints for general nonlinear vehicle models. Moreover, the CBF safety constraints are incorporated into a nonlinear model predictive control (NMPC) framework. The advantage is twofold. Firstly, both vehicle driving safety and comfort performance can be improved. Secondly, it helps to reduce computational burden in real time MPC implementation. The proposed algorithm is validated and compared with conventional NMPC in several safetycritical scenarios including sudden objects and road boundaries avoidance, showing improvements in both safety and smooth driving. The validation is conducted based on a co-simulation of two softwares, Siemens Simcenter Amesim and Prescan, which simulate high fidelity vehicle dynamics and traffic environment, respectively},
author = {{Tong Duy Son}, Quan Nguyen},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/MPC/mail{\_}son/CBFMPC.pdf:pdf},
number = {August},
pages = {7},
title = {{Safety-Critical Control for Non-affine Nonlinear Systems with Application on Autonomous Vehicle}},
url = {file:///home/daniele/Downloads/ADAS{\_}SafetyCriticalControl.pdf},
year = {2019}
}
@article{Turner1999,
abstract = {Relationships between vehicle motion and passenger sickness have been investigated in a survey of 3256 passengers travelling on 56 mainland UK bus or coach journeys. Vehicle motion was measured throughout all journeys, yielding over 110 h of six-axis coach motion data from five types of coach and 17 different drivers. Overall, 28.4{\%} of passengers reported feelings of illness, 12.8{\%} reported nausea and 1.7{\%} reported vomiting during coach travel. Passenger nausea and illness ratings increased with increased exposure to lateral coach motion at low frequencies ({\textless} 0.5 Hz). Motion in other axes correlated less well with sickness, although there were some intercorrelations between the motions in the different axes. Sickness levels among passengers were greater with drivers who drove to produce higher average magnitudes of fore-and-aft and lateral vehicle motion. Nausea occurrence was greater on routes classified as being predominantly cross-country where magnitudes of lateral vehicle motion were significantly higher. Lateral motion and motion sickness increased from the front to the rear of each vehicle. No significant differences in sickness were found between the five different vehicle types used in the study. The applicability of a motion sickness dose model to these data is discussed.},
author = {Turner, Mark and Griffin, Michael J.},
doi = {10.1080/001401399184730},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/Comfort/Motion sickness in public road transport the effect of driver route and vehicle.pdf:pdf},
isbn = {0014013991},
issn = {00140139},
journal = {Ergonomics},
keywords = {Acceleration,Driver,Frequency,Motion sickness,Nausea,Vehicle},
number = {12},
pages = {1646--1664},
title = {{Motion sickness in public road transport: The effect of driver, route and vehicle}},
volume = {42},
year = {1999}
}
@article{Abbeel2004,
abstract = {We consider learning in a Markov decision process where we are not explicitly given a reward function, but where instead we can observe an expert demonstrating the task that we want to learn to perform. This setting is useful in applications (such as the task of driving) where it may be difficult to write down an explicit reward function specifying exactly how different desiderata should be traded off. We think of the expert as trying to maximize a reward function that is expressible as a linear combination of known features, and give an algorithm for learning the task demonstrated by the expert. Our. algorithm is based on using "inverse reinforcement learning" to try to recover the unknown reward function. We show that our algorithm terminates in a small number of iterations, and that even though we may never recover the expert's reward function, the policy output by the algorithm will attain performance close to that of the expert, where here performance is measured with respect to the expert's unknown reward function.},
author = {Abbeel, Pieter and Ng, Andrew Y.},
doi = {10.1145/1015330.1015430},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/Inverse reinforced learning/Apprenticeship learning@stanford.pdf:pdf},
isbn = {1581138385},
journal = {Proceedings, Twenty-First International Conference on Machine Learning, ICML 2004},
pages = {1--8},
title = {{Apprenticeship learning via inverse reinforcement learning}},
year = {2004}
}
@article{Patrinos2019,
author = {Patrinos, Panos},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/MPC/MPC class/1intro.pdf:pdf},
title = {{Model Predictive Control - Lecture Notes}},
year = {2019}
}
@article{Mercy2018,
author = {Mercy, Tim},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/Stick{\_}Tong{\_}kickoff/thesis{\_}MercyTim.pdf:pdf},
number = {September},
title = {{Spline-Based Motion Planning for Autonomous Mechatronic Systems}},
year = {2018}
}
@article{Kuderer2015a,
abstract = {It is expected that autonomous vehicles capable of driving without human supervision will be released to market within the next decade. For user acceptance, such vehicles should not only be safe and reliable, they should also provide a comfortable user experience. However, individual perception of comfort may vary considerably among users. Whereas some users might prefer sporty driving with high accelerations, others might prefer a more relaxed style. Typically, a large number of parameters such as acceleration profiles, distances to other cars, speed during lane changes, etc., characterize a human driver's style. Manual tuning of these parameters may be a tedious and error-prone task. Therefore, we propose a learning from demonstration approach that allows the user to simply demonstrate the desired style by driving the car manually. We model the individual style in terms of a cost function and use feature-based inverse reinforcement learning to find the model parameters that fit the observed style best. Once the model has been learned, it can be used to efficiently compute trajectories for the vehicle in autonomous mode. We show that our approach is capable of learning cost functions and reproducing different driving styles using data from real drivers.},
author = {Kuderer, Markus and Gulati, Shilpa and Burgard, Wolfram},
doi = {10.1109/ICRA.2015.7139555},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Leuven/Thesis/References/obesvation{\_}based{\_}learning/Learning driving styles for autonomous driving.pdf:pdf},
issn = {10504729},
journal = {Proceedings - IEEE International Conference on Robotics and Automation},
keywords = {Autonomous Navigation,Learning and Adaptive Systems},
number = {June},
pages = {2641--2646},
title = {{Learning driving styles for autonomous vehicles from demonstration}},
volume = {2015-June},
year = {2015}
}
@article{Gillis2019,
author = {Gillis, Joris},
file = {:C$\backslash$:/Users/t2vosx/OneDrive/Documenten/Python{\_}guardian{\_}angle/Thesis/CasADi{\_}sessions/Day 2/7.ocp/presentation.pdf:pdf},
number = {November},
pages = {1--42},
title = {{Ya Coda course presentation}},
year = {2019}
}
